{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\ajoshi\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "D:\\Users\\ajoshi\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_selection.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "\n",
    "import eli5\n",
    "import eli5.sklearn.explain_prediction as expred\n",
    "import eli5.sklearn.explain_weights as exweight\n",
    "import eli5.xgboost as xgb\n",
    "import eli5.lightgbm as lgbm\n",
    "import eli5.catboost as catb\n",
    "from pdpbox import pdp, get_dataset, info_plots\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from yellowbrick.regressor import PredictionError\n",
    "from yellowbrick.regressor import AlphaSelection\n",
    "from yellowbrick.regressor import CooksDistance\n",
    "#from yellowbrick.classifier import *\n",
    "import shap\n",
    "\n",
    "from pycaret.regression import *\n",
    "from pycaret.classification import *\n",
    "import explainer as ex_intr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Interpreter:\n",
    "    def __init__(self, package):\n",
    "\n",
    "        # data unloading\n",
    "        self.model = package[0]\n",
    "        self.X_train = package[1]\n",
    "        self.X_test = package[2]\n",
    "        self.y_train = package[3]\n",
    "        self.y_test = package[4]\n",
    "        self.base_features = package[5]\n",
    "        self.model_type = str(str(type(self.model)).split('.')[-1].split(\"'\")[0])\n",
    "        print(self.model_type)\n",
    "        self.regressors = ['LinearRegression', 'Ridge', 'RandomForestRegressor', 'DecisionTreeRegressor', 'XGBRegressor',\n",
    "                           'LGBMRegressor', 'CatBoostRegressor']\n",
    "        self.classifiers = ['LogisticRegression', 'LogisticRegressionCV', 'RidgeClassifier','RandomForestClassifier', \n",
    "                            'DecisionTreeClassifier', 'QuadraticDiscriminantAnalysis']\n",
    "\n",
    "    def interprete_model(self, interpretation_tool, **kwargs):\n",
    "        \n",
    "        if(self.model_type in self.regressors):\n",
    "            print(\"model == Random Forest\")\n",
    "            plot_type = kwargs.get('plotname', 'residuals')\n",
    "        elif(self.model_type in self.classifiers):\n",
    "            plot_type = kwargs.get('plotname', 'auc')\n",
    "\n",
    "        print('Model name: ' + str(self.model_type))\n",
    "        # for linear regressors\n",
    "        if (self.model_type in self.regressors) and interpretation_tool == 'eli5':  # for eli5\n",
    "            print('\\n\\nPlotting ELI5 interpretations...\\n\\n')\n",
    "            if (self.model_type == 'LinearRegression') or (self.model_type == 'Ridge'):\n",
    "\n",
    "                ex_intr.Explainer('EPLR')\n",
    "                display(\n",
    "                    expred.explain_prediction_linear_regressor(self.model, doc=self.X_train.values[randint(0, 100)]))\n",
    "\n",
    "                ex_intr.Explainer('EPLRW')\n",
    "                display(exweight.explain_linear_regressor_weights(self.model))\n",
    "\n",
    "                # explain permutation importance\n",
    "                perm = eli5.sklearn.PermutationImportance(self.model, random_state=1).fit(self.X_train, self.y_train)\n",
    "                ex_intr.Explainer('ESW')\n",
    "                display(eli5.show_weights(perm, feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "            elif self.model_type == 'RandomForestRegressor':\n",
    "                # explain tree regressor feature importance\n",
    "                \n",
    "                # show predictions\n",
    "                display(eli5.show_prediction(self.model, self.X_test.iloc[10], show_feature_values=True))\n",
    "\n",
    "                ex_intr.Explainer('EPTR')\n",
    "                display(expred.explain_prediction_tree_regressor(self.model, doc=self.X_train.values[randint(0, 100)],\n",
    "                                                                 feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "                # explain feature importance without permutation\n",
    "                ex_intr.Explainer('EFRF')\n",
    "                display(exweight.explain_rf_feature_importance(self.model, feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "                # explain with permutation, the feature importance\n",
    "                perm = eli5.sklearn.PermutationImportance(self.model, random_state=1).fit(self.X_train, self.y_train)\n",
    "                ex_intr.Explainer('ESW')\n",
    "                display(eli5.show_weights(perm, feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "            elif self.model_type == 'XGBRegressor':\n",
    "                print('Hello')\n",
    "                display(xgb.explain_weights_xgboost(self.model))\n",
    "\n",
    "            elif self.model_type == 'LGBMRegressor':\n",
    "                print('Hello')\n",
    "                display(lgbm.explain_weights_lightgbm(self.model))\n",
    "\n",
    "            elif self.model_type == 'CatBoostRegressor':\n",
    "                display(catb.explain_weights_catboost(self.model))\n",
    "\n",
    "\n",
    "        elif (self.model_type in self.classifiers) and interpretation_tool == 'eli5':\n",
    "            if self.model_type == 'LogisticRegression' or self.model_type == 'QuadraticDiscriminantAnalysis':\n",
    "                display(\n",
    "                    expred.explain_prediction_linear_classifier(self.model, doc=self.X_train.values[randint(0, 100)]))\n",
    "                display(exweight.explain_linear_classifier_weights(self.model))\n",
    "                # explain permutation importance\n",
    "                perm = eli5.sklearn.PermutationImportance(self.model, random_state=1).fit(self.X_train, self.y_train)\n",
    "                display(eli5.show_weights(perm, feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "            elif self.model_type == 'DecisionTreeClassifier':\n",
    "                display(expred.explain_prediction_tree_classifier(self.model, doc=self.X_train.values[randint(0, 100)]))\n",
    "                display(exweight.explain_decision_tree(self.model))\n",
    "                # explain permutation importance\n",
    "                perm = eli5.sklearn.PermutationImportance(self.model, random_state=1).fit(self.X_train, self.y_train)\n",
    "                display(eli5.show_weights(perm, feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "            elif self.model_type == 'RandomForestClassifier':\n",
    "                display(expred.explain_prediction_tree_classifier(self.model, doc=self.X_train.values[randint(0, 100)]))\n",
    "                display(exweight.explain_rf_feature_importance(self.model))\n",
    "                display(eli5.show_weights(self.model))\n",
    "                # explain permutation importance\n",
    "                perm = eli5.sklearn.PermutationImportance(self.model, random_state=1).fit(self.X_train, self.y_train)\n",
    "                display(eli5.show_weights(perm, feature_names=self.X_train.columns.tolist()))\n",
    "\n",
    "            elif self.model_type == 'XGBClassifier':\n",
    "                print('Hello')\n",
    "                display(xgb.explain_weights_xgboost(self.model))\n",
    "\n",
    "            elif self.model_type == 'LGBMClassifier':\n",
    "                print('Hello')\n",
    "                display(lgbm.explain_weights_lightgbm(self.model))\n",
    "\n",
    "            elif self.model_type == 'CatBoostClassifier':\n",
    "                display(catb.explain_weights_catboost(self.model))\n",
    "\n",
    "        elif (\n",
    "                self.model_type in self.regressors or self.model_type in self.classifiers) and interpretation_tool == 'pdpbox':\n",
    "            print('\\n\\nPlotting PDP box interpretations...\\n\\n')\n",
    "            ex_intr.Explainer('PDPB')\n",
    "            for b_feature in self.base_features:\n",
    "                pdp_goals = pdp.pdp_isolate(model=self.model, dataset=self.X_train, model_features=self.base_features,\n",
    "                                            feature=b_feature)\n",
    "                pdp.pdp_plot(pdp_goals, b_feature)\n",
    "                plt.show()\n",
    "\n",
    "        elif (\n",
    "                self.model_type in self.regressors or self.model_type in self.classifiers) and interpretation_tool == 'shap':\n",
    "            print('\\n\\nPlotting SHAP interpretations...\\n\\n')\n",
    "            shap.initjs()\n",
    "            explainer = shap.Explainer(self.model)\n",
    "            shap_values = explainer(self.X_train)\n",
    "            shap_values.base_value = shap_values.base_values[0]\n",
    "\n",
    "            print(\"\\n\\nPlotting SHAP Summary Plot...\\n\")\n",
    "            ex_intr.Explainer('SSP')\n",
    "            shap.summary_plot(shap_values)\n",
    "\n",
    "            print(\"\\n\\nPlotting SHAP Decision Plot...\\n\")\n",
    "            ex_intr.Explainer('SDP')\n",
    "            shap.decision_plot(shap_values.base_values[0], shap_values.values)\n",
    "\n",
    "            row_num = randint(0, 100)\n",
    "            print(\"\\n\\nPlotting SHAP Multi-Output Decision Plot for row \" + str(row_num) + \"...\\n\")\n",
    "            ex_intr.Explainer('MODP')\n",
    "            shap.multioutput_decision_plot([shap_values.base_values.tolist()], [shap_values.values.tolist()], row_num)\n",
    "\n",
    "            # replace 0 with index of feature to plot\n",
    "            print(\"\\n\\nPlotting SHAP Monitoring Plot...\\n\")\n",
    "            shap.monitoring_plot(0, shap_values.values, shap_values.base_values, feature_names=self.base_features)\n",
    "\n",
    "\n",
    "        elif (self.model_type in self.regressors) and interpretation_tool == 'yellowbrick':\n",
    "            print(\"\\n\\nPlotting Yellowbrick Interpretations...\\n\\n\")\n",
    "\n",
    "            print(\"\\n\\nPlotting Yellowbrick ResidualsPlot...\\n\")\n",
    "            ex_intr.Explainer('YBRP')\n",
    "            visualizer = ResidualsPlot(self.model)\n",
    "            visualizer.fit(self.X_train, self.y_train)\n",
    "            visualizer.score(self.X_test, self.y_test)\n",
    "            visualizer.show()\n",
    "\n",
    "            print(\"\\n\\nPlotting Yellowbrick Prediction Error Plot...\\n\")\n",
    "            ex_intr.Explainer('YBPEP')\n",
    "            prediction_error_visualizer = PredictionError(self.model)\n",
    "            prediction_error_visualizer.fit(self.X_train, self.y_train)\n",
    "            prediction_error_visualizer.score(self.X_test, self.y_test)\n",
    "            prediction_error_visualizer.show()\n",
    "\n",
    "        elif (self.model_type in self.classifiers) and interpretation_tool == 'yellowbrick':\n",
    "            print(\"\\n\\nPlotting Yellowbrick Interpretations...\\n\\n\")\n",
    "\n",
    "            print(\"\\n\\nPlotting Yellowbrick ClassificationReport...\\n\")\n",
    "            visualizer = ClassificationReport(self.model, support=True)\n",
    "            visualizer.fit(self.X_train, self.y_train)  # Fit the visualizer and the model\n",
    "            visualizer.score(self.X_test, self.y_test)  # Evaluate the model on the test data\n",
    "            visualizer.show()\n",
    "\n",
    "        elif (self.model_type in self.regressors) and interpretation_tool == 'pycaret':\n",
    "            plots = ['residuals', 'error', 'rfe', 'learning', 'vc', 'manifold', 'parameter', 'feature',\n",
    "                     'parameter', 'tree']\n",
    "\n",
    "            print(\"Plotting PyCaret Interpretations...\")\n",
    "            if(plot_type in plots):\n",
    "                ex_intr.Explainer(plot_type)\n",
    "                plot_model(self.model, plot_type)\n",
    "            else:\n",
    "                print(\"No such plot available.\")\n",
    "\n",
    "        elif (self.model_type in self.classifiers) and interpretation_tool == 'pycaret':\n",
    "            plots = ['auc', 'threshold', 'pr', 'confusion_matrix', 'error', 'class_report', 'boundary', 'rfe', 'learning',\n",
    "                     'manifold', 'calibration', 'vc', 'dimension', 'feature', 'parameter', 'lift', 'gain', 'tree']\n",
    "\n",
    "            print(\"Plotting PyCaret Interpretations...\")\n",
    "            if(plot_type in plots):\n",
    "                ex_intr.Explainer(str(plot_type))\n",
    "                plot_model(self.model, plot_type)\n",
    "            else:\n",
    "                print(\"No such plot available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationMetrics:\n",
    "    def __init__(self, package, metric_name=None,beta=None,pred_prob=None):\n",
    "        self.model = package[0]\n",
    "        self.y_true = package[1]\n",
    "        self.y_pred = package[2]\n",
    "        self.beta = beta\n",
    "        # print(self.y_true)\n",
    "        # self.metrics_list.append(\n",
    "        global flag\n",
    "        flag = 1\n",
    "        if metric_name is None:\n",
    "            self._run_all()\n",
    "        elif metric_name == 'accuracy':\n",
    "            self._accuracy(self.y_true, self.y_pred)\n",
    "        # self.metrics_list.append(\n",
    "        elif metric_name == 'precision':\n",
    "            self._precision(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'recall':\n",
    "            self._recall(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'f1_score':\n",
    "            self._f1_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'fbeta_score':\n",
    "            if beta is not None:\n",
    "                self._fbeta_score(self.y_true, self.y_pred, beta)\n",
    "            else:\n",
    "                print('\\033[1m' + '\\nF-Beta Score:' + '\\033[1m' + '\\nBeta Value Required for F-Beta score')\n",
    "                f_beta = ['F-Beta Score', 'Beta Value Required']\n",
    "                MetricList.append(f_beta)\n",
    "        elif metric_name == 'auc':\n",
    "            self._auc(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'matthews_corrcoef':\n",
    "            self._matthews_corrcoef(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'accuracy':\n",
    "            self._hamming_loss(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'hamming_loss':\n",
    "            self._logloss(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'zero_one_loss':\n",
    "            self._zero_one_loss(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'cohen_kappa_score':\n",
    "            self._cohen_kappa_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'gini_score':\n",
    "            if pred_prob is not None:\n",
    "                self._gini_score(pred_prob)\n",
    "            else:\n",
    "                display('Gini Score Required prediction probability')\n",
    "        else:\n",
    "            display('Invalid Metric Name')\n",
    "            flag = 0\n",
    "            \n",
    "    def _run_all(self):\n",
    "        self._accuracy(self.y_true, self.y_pred)\n",
    "        # self.metrics_list.append(\n",
    "        self._precision(self.y_true, self.y_pred)\n",
    "        self._recall(self.y_true, self.y_pred)\n",
    "        self._f1_score(self.y_true, self.y_pred)\n",
    "        self._auc(self.y_true, self.y_pred)\n",
    "        self._matthews_corrcoef(self.y_true, self.y_pred)\n",
    "        self._hamming_loss(self.y_true, self.y_pred)\n",
    "        self._logloss(self.y_true, self.y_pred)\n",
    "        self._zero_one_loss(self.y_true, self.y_pred)\n",
    "        self._cohen_kappa_score(self.y_true, self.y_pred)\n",
    "\n",
    "    # @staticmethod\n",
    "    # def _confusion_matrix(y_true, y_pred):\n",
    "    #     print('conf')\n",
    "    #     return metrics.confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def _gini_score(pred_prob):\n",
    "        #Logic For Gini Score\n",
    "        sum_pp=sum(pred_prob)\n",
    "        val=sum_pp/sum(sum_pp)\n",
    "        gini = ['Gini Score',1-sum(val*val)]\n",
    "        MetricList.append(gini)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _accuracy(y_true, y_pred):\n",
    "        # for i in range(11):\n",
    "        #     print(y_true[i],\"\\t\",y_pred[i])\n",
    "        # print(metrics.accuracy_score(y_true, y_pred))\n",
    "        # print('\\033[1m' + '\\nAccuracy Score:', metrics.accuracy_score(y_true, y_pred), '\\033[0m')\n",
    "        accuracy = ['Accuracy', metrics.accuracy_score(y_true, y_pred)]\n",
    "        MetricList.append(accuracy)\n",
    "\n",
    "    @staticmethod\n",
    "    def _precision(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nPrecision:', metrics.precision_score(y_true, y_pred), '\\033[0m')\n",
    "        precision = ['Precision', metrics.precision_score(y_true, y_pred, zero_division=0)]\n",
    "        MetricList.append(precision)\n",
    "\n",
    "    @staticmethod\n",
    "    def _recall(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nRecall:', metrics.recall_score(y_true, y_pred), '\\033[0m')\n",
    "        recall = ['Recall', metrics.recall_score(y_true, y_pred)]\n",
    "        MetricList.append(recall)\n",
    "\n",
    "    @staticmethod\n",
    "    def _f1_score(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nF1 Score:', metrics.f1_score(y_true, y_pred), '\\033[0m')\n",
    "        f1_score = ['F1 Score', metrics.f1_score(y_true, y_pred)]\n",
    "        MetricList.append(f1_score)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fbeta_score(y_true, y_pred, beta):\n",
    "        # print('\\033[1m' + '\\nF-Beta Score:', metrics.fbeta_score(y_true, y_pred, beta=beta), '\\033[0m')\n",
    "        f_beta = ['F-Beta Score', metrics.fbeta_score(y_true, y_pred, beta=beta)]\n",
    "        MetricList.append(f_beta)\n",
    "\n",
    "    @staticmethod\n",
    "    def _auc(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nAUC:', metrics.roc_auc_score(y_true, y_pred), '\\033[0m')\n",
    "        auc = ['AUC Score', metrics.roc_auc_score(y_true, y_pred)]\n",
    "        MetricList.append(auc)\n",
    "\n",
    "    @staticmethod\n",
    "    def _classification_report(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nClassification Report:', metrics.classification_report(y_true, y_pred), '\\033[0m')\n",
    "        class_report = ['Classification Report', metrics.classification_report(y_true, y_pred)]\n",
    "        MetricList.append(class_report)\n",
    "\n",
    "    @staticmethod\n",
    "    def _matthews_corrcoef(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nMatthews Correlation Coefficient:', metrics.matthews_corrcoef(y_true, y_pred), '\\033[0m')\n",
    "        matthews = ['Matthews CorrCoef', metrics.matthews_corrcoef(y_true, y_pred)]\n",
    "        MetricList.append(matthews)\n",
    "\n",
    "    @staticmethod\n",
    "    def _hamming_loss(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nHamming Loss:', metrics.hamming_loss(y_true, y_pred), '\\033[0m')\n",
    "        hamming_loss = ['Hamming Loss', metrics.hamming_loss(y_true, y_pred)]\n",
    "        MetricList.append(hamming_loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def _logloss(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nLog Loss:', metrics.log_loss(y_true, y_pred), '\\033[0m')\n",
    "        log_loss = ['Log Loss', metrics.log_loss(y_true, y_pred)]\n",
    "        MetricList.append(log_loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def _zero_one_loss(y_true, y_pred):\n",
    "        # print('\\033[1m' + '\\nZero One Loss:', metrics.zero_one_loss(y_true, y_pred), '\\033[0m')\n",
    "        zero_one_loss = ['Zero One Loss', metrics.zero_one_loss(y_true, y_pred)]\n",
    "        MetricList.append(zero_one_loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cohen_kappa_score(y_true, y_pred):\n",
    "        cohen = ['Cohen Kappa Score', metrics.cohen_kappa_score(y_true, y_pred)]\n",
    "        MetricList.append(cohen)\n",
    "        # print('\\033[1m' + '\\nCohen Kappa:', metrics.cohen_kappa_score(y_true, y_pred), '\\033[0m')\n",
    "    #\n",
    "    # @staticmethod\n",
    "    # def _explain(metric_name):\n",
    "\n",
    "\n",
    "class RegressionMetrics:\n",
    "    def __init__(self, package, metric_name=None):\n",
    "        self.model = package[0]\n",
    "        self.y_true = package[1]\n",
    "        self.y_pred = package[2]\n",
    "        global flag\n",
    "        flag = 1\n",
    "        if metric_name is None:\n",
    "            self._run_all()\n",
    "        elif metric_name == 'explained_variance_score':\n",
    "            self._explained_variance_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'r2_score':\n",
    "            self._r2_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'mean_absolute_error':\n",
    "            self._mean_absolute_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'mean_absolute_percentage_error':\n",
    "            self._mean_absolute_percentage_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'median_absolute_error':\n",
    "            self._median_absolute_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'mean_squared_error':\n",
    "            self._mean_squared_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'root_mean_squared_error':\n",
    "            self._root_mean_squared_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'mean_squared_log_error':\n",
    "            self._mean_squared_log_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'root_mean_squared_log_error':\n",
    "            self._root_mean_squared_log_error(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'max_error':\n",
    "            self._max_error(self.y_true, self.y_pred)\n",
    "        else:\n",
    "            display('Invalid Metric Name')\n",
    "            flag = 0\n",
    "\n",
    "    def _run_all(self):\n",
    "        self._explained_variance_score(self.y_true, self.y_pred)\n",
    "        self._r2_score(self.y_true, self.y_pred)\n",
    "        self._mean_absolute_error(self.y_true, self.y_pred)\n",
    "        self._mean_absolute_percentage_error(self.y_true, self.y_pred)\n",
    "        self._median_absolute_error(self.y_true, self.y_pred)\n",
    "        self._mean_squared_error(self.y_true, self.y_pred)\n",
    "        self._root_mean_squared_error(self.y_true, self.y_pred)\n",
    "        self._mean_squared_log_error(self.y_true, self.y_pred)\n",
    "        self._root_mean_squared_log_error(self.y_true, self.y_pred)\n",
    "        self._max_error(self.y_true, self.y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def _explained_variance_score(y_true, y_pred):\n",
    "        op = ['Explained Variance Score', metrics.explained_variance_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _r2_score(y_true, y_pred):\n",
    "        op = ['R2 Score', metrics.r2_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_absolute_error(y_true, y_pred):\n",
    "        op = ['Mean Absolute Error', metrics.mean_absolute_error(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_absolute_percentage_error(y_true, y_pred):\n",
    "        x = np.abs(y_true - y_pred) / np.abs(y_true)\n",
    "        x = (100 / len(y_true)) * x\n",
    "        x = np.mean(x)\n",
    "        x = str(x)\n",
    "        print(type(x))\n",
    "        op = ['Mean Absolute Percentage Error', x]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _median_absolute_error(y_true, y_pred):\n",
    "        op = ['Median Absolute Error', metrics.median_absolute_error(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_squared_error(y_true, y_pred):\n",
    "        op = ['Mean Squared Error', metrics.mean_squared_error(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _root_mean_squared_error(y_true, y_pred):\n",
    "        op = ['Root Mean Squared Error', np.sqrt(metrics.mean_squared_error(y_true, y_pred))]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mean_squared_log_error(y_true, y_pred):\n",
    "        op = ['Mean Squared Log Error', metrics.explained_variance_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _root_mean_squared_log_error(y_true, y_pred):\n",
    "        op = ['Root Mean Squared Log Error', np.sqrt(metrics.mean_squared_log_error(y_true, y_pred))]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _max_error(y_true, y_pred):\n",
    "        op = ['Max Error', metrics.max_error(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "\n",
    "class ClusteringMetrics:\n",
    "    def __init__(self, package, samples, metric_name=None):\n",
    "        self.model = package[0]\n",
    "        self.y_true = package[1]\n",
    "        self.y_pred = package[2]\n",
    "        self.samples = samples\n",
    "        global flag\n",
    "        flag = 1\n",
    "        print(metric_name)\n",
    "        if metric_name is None:\n",
    "            self._run_all()\n",
    "        elif metric_name == 'mutual_info_score':\n",
    "            self._mutual_info_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'normalized_mutual_info_score':\n",
    "            self._normalized_mutual_info_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'adjusted_mutual_info_score':\n",
    "            self._adjusted_mutual_info_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'adjusted_rand_score':\n",
    "            self._adjusted_rand_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'fowlkes_mallows_score':\n",
    "            self._fowlkes_mallows_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'homogeneity_score':\n",
    "            self._homogeneity_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'completeness_score':\n",
    "            self._completeness_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'v_measure_score':\n",
    "            self._v_measure_score(self.y_true, self.y_pred)\n",
    "        elif metric_name == 'silhouette_score':\n",
    "            self._silhouette_score(self.samples, self.y_pred)\n",
    "        elif metric_name == 'silhouette_samples':\n",
    "            self._silhouette_samples(self.samples, self.y_pred)\n",
    "        elif metric_name == 'davies_bouldin_score':\n",
    "            self._davies_bouldin_score(self.samples, self.y_pred)\n",
    "        elif metric_name == 'calinski_harabasz_score':\n",
    "            self._calinski_harabasz_score(self.samples, self.y_pred)\n",
    "        else:\n",
    "            display('Invalid Metric Name')\n",
    "            flag = 0\n",
    "\n",
    "\n",
    "    def _run_all(self):\n",
    "        self._mutual_info_score(self.y_true, self.y_pred)\n",
    "        self._normalized_mutual_info_score(self.y_true, self.y_pred)\n",
    "        self._adjusted_mutual_info_score(self.y_true, self.y_pred)\n",
    "        self._adjusted_rand_score(self.y_true, self.y_pred)\n",
    "        self._fowlkes_mallows_score(self.y_true, self.y_pred)\n",
    "        self._homogeneity_score(self.y_true, self.y_pred)\n",
    "        self._completeness_score(self.y_true, self.y_pred)\n",
    "        self._v_measure_score(self.y_true, self.y_pred)\n",
    "        self._silhouette_score(self.samples, self.y_pred)\n",
    "        self._davies_bouldin_score(self.samples, self.y_pred)\n",
    "        self._calinski_harabasz_score(self.samples, self.y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def _silhouette_score(y_true, y_pred):\n",
    "        op = ['Silhouette Score', metrics.silhouette_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _silhouette_samples(y_true, y_pred):\n",
    "        op = ['Silhouette Sample', metrics.silhouette_samples(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _mutual_info_score(y_true, y_pred):\n",
    "        op = ['Mutual Info Score', metrics.mutual_info_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalized_mutual_info_score(y_true, y_pred):\n",
    "        op = ['Normalized Mutual Info Score', metrics.normalized_mutual_info_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _adjusted_mutual_info_score(y_true, y_pred):\n",
    "        op = ['Adjusted Mutual Info Score', metrics.adjusted_mutual_info_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _adjusted_rand_score(y_true, y_pred):\n",
    "        op = ['Adjusted Rand Score', metrics.adjusted_rand_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _fowlkes_mallows_score(y_true, y_pred):\n",
    "        op = ['Fowlkes Mallows Score', metrics.fowlkes_mallows_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _homogeneity_score(y_true, y_pred):\n",
    "        op = ['Homogeneity Score', metrics.homogeneity_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _completeness_score(y_true, y_pred):\n",
    "        op = ['Completeness Score', metrics.completeness_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _v_measure_score(y_true, y_pred):\n",
    "        op = ['V Measure Score', metrics.v_measure_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _davies_bouldin_score(y_true, y_pred):\n",
    "        op = ['Davies Bouldin Score', metrics.davies_bouldin_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "    @staticmethod\n",
    "    def _calinski_harabasz_score(y_true, y_pred):\n",
    "        op = ['Calinski Harabasz Score', metrics.calinski_harabasz_score(y_true, y_pred)]\n",
    "        MetricList.append(op)\n",
    "\n",
    "\n",
    "# print(model,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load Dataset\n",
    "\n",
    "dataset = pd.read_csv('cars1.csv')\n",
    "#print(dataset)\n",
    "y = dataset.selling_price\n",
    "\n",
    "base_features = ['transmission','year','km_driven','mileage','engine','max_power','seats']\n",
    "\n",
    "X = dataset[base_features]\n",
    "\n",
    "#print(X)\n",
    "#print(y)\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "first_model = RandomForestRegressor(n_estimators=50, random_state=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor\n"
     ]
    }
   ],
   "source": [
    "# packaging the data for interpretation\n",
    "interpretation_package = [first_model, X_train, X_test, y_train, y_test, base_features]\n",
    "interpreter = Interpreter(interpretation_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter.interprete_model('eli5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter.interprete_model('pdpbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter.interprete_model('shap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpreter.interprete_model('yellowbrick')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model == Random Forest\n",
      "Model name: RandomForestRegressor\n",
      "Plotting PyCaret Interpretations...\n",
      "\n",
      "\n",
      "• Learning Curve: Line plot of learning (y-axis) over experience (x-axis).\n",
      "\n",
      "• There are three common dynamics that you are likely to observe in learning curves; they are:\n",
      "\t\t• Underfit.\n",
      "\t\t• Overfit.\n",
      "\t\t• Good Fit.\n",
      "\n",
      "• An underfit model can be identified from the learning curve of the training loss only.\n",
      "It may show a flat line or noisy values of relatively high loss, indicating that the model was unable to learn the training dataset at all.\n",
      "\n",
      "• A plot of learning curves shows overfitting if the plot of training loss continues to decrease with experience OR\n",
      "if the plot of validation loss decreases to a point and begins increasing again.\n",
      "\n",
      "• A good fit is identified by a training and validation loss that decreases to a point of stability with a minimal gap between\n",
      "the two final loss values.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Plot Not Available. Please see docstring for list of available Plots.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-6cba42c9e59b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minterpreter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterprete_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"pycaret\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplotname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"learning\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-a725927130cc>\u001b[0m in \u001b[0;36minterprete_model\u001b[1;34m(self, interpretation_tool, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mplots\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mex_intr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m                 \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such plot available.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pycaret\\classification.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(estimator, plot, scale, save, fold, fit_kwargs, groups, use_train_data, verbose)\u001b[0m\n\u001b[0;32m   1529\u001b[0m     \"\"\"\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m     return pycaret.internal.tabular.plot_model(\n\u001b[0m\u001b[0;32m   1532\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m         \u001b[0mplot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pycaret\\internal\\tabular.py\u001b[0m in \u001b[0;36mplot_model\u001b[1;34m(estimator, plot, scale, save, fold, fit_kwargs, groups, feature_name, label, use_train_data, verbose, system, display)\u001b[0m\n\u001b[0;32m   5705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mplot\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_available_plots\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5707\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   5708\u001b[0m             \u001b[1;34m\"Plot Not Available. Please see docstring for list of available Plots.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5709\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Plot Not Available. Please see docstring for list of available Plots."
     ]
    }
   ],
   "source": [
    "interpreter.interprete_model(\"pycaret\", plotname=\"learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
